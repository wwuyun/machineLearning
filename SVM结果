实验一结果：
              precision    recall  f1-score   support

           0       0.68      0.75      0.71      2076
           1       0.91      0.93      0.92      2108
           2       0.68      0.62      0.65      2094
           3       0.63      0.64      0.63      2148
           4       0.61      0.66      0.64      2070
           5       0.90      0.86      0.88      2144
           6       0.54      0.45      0.49      2131
           7       0.86      0.91      0.88      2076
           8       0.88      0.92      0.90      2017
           9       0.93      0.92      0.92      2136

    accuracy                           0.76     21000
   macro avg       0.76      0.77      0.76     21000
weighted avg       0.76      0.76      0.76     21000

Accuracy: 0.7644285714285715


实验二结果：
Using SVM with linear kernel:
              precision    recall  f1-score   support

           0       0.68      0.75      0.71      2076
           1       0.91      0.93      0.92      2108
           2       0.68      0.62      0.65      2094
           3       0.63      0.64      0.63      2148
           4       0.61      0.66      0.64      2070
           5       0.90      0.86      0.88      2144
           6       0.54      0.45      0.49      2131
           7       0.86      0.91      0.88      2076
           8       0.88      0.92      0.90      2017
           9       0.93      0.92      0.92      2136

    accuracy                           0.76     21000
   macro avg       0.76      0.77      0.76     21000
weighted avg       0.76      0.76      0.76     21000

Using SVM with poly kernel:
              precision    recall  f1-score   support

           0       0.72      0.77      0.74      2076
           1       0.93      0.94      0.93      2108
           2       0.71      0.67      0.69      2094
           3       0.69      0.68      0.68      2148
           4       0.64      0.70      0.67      2070
           5       0.93      0.91      0.92      2144
           6       0.60      0.51      0.55      2131
           7       0.89      0.92      0.91      2076
           8       0.92      0.94      0.93      2017
           9       0.95      0.93      0.94      2136

    accuracy                           0.80     21000
   macro avg       0.80      0.80      0.80     21000
weighted avg       0.80      0.80      0.80     21000

Using SVM with rbf kernel:
              precision    recall  f1-score   support

           0       0.71      0.77      0.74      2076
           1       0.92      0.94      0.93      2108
           2       0.71      0.66      0.68      2094
           3       0.67      0.67      0.67      2148
           4       0.64      0.69      0.66      2070
           5       0.92      0.90      0.91      2144
           6       0.59      0.50      0.54      2131
           7       0.88      0.92      0.90      2076
           8       0.91      0.94      0.92      2017
           9       0.94      0.92      0.93      2136

    accuracy                           0.79     21000
   macro avg       0.79      0.79      0.79     21000
weighted avg       0.79      0.79      0.79     21000



实验三结果：
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten (Flatten)            (None, 784)               0         
_________________________________________________________________
dense (Dense)                (None, 128)               100480    
_________________________________________________________________
dropout (Dropout)            (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 10)                1290      
=================================================================
Total params: 101,770
Trainable params: 101,770
Non-trainable params: 0
_________________________________________________________________
Epoch 1/10
1532/1532 [==============================] - 3s 2ms/step - loss: 0.5483 - accuracy: 0.8066 - val_loss: 0.4210 - val_accuracy: 0.8450
Epoch 2/10
1532/1532 [==============================] - 3s 2ms/step - loss: 0.4138 - accuracy: 0.8515 - val_loss: 0.3715 - val_accuracy: 0.8671
Epoch 3/10
1532/1532 [==============================] - 3s 2ms/step - loss: 0.3762 - accuracy: 0.8637 - val_loss: 0.3628 - val_accuracy: 0.8680
Epoch 4/10
1532/1532 [==============================] - 3s 2ms/step - loss: 0.3588 - accuracy: 0.8678 - val_loss: 0.3474 - val_accuracy: 0.8728
Epoch 5/10
1532/1532 [==============================] - 2s 1ms/step - loss: 0.3405 - accuracy: 0.8756 - val_loss: 0.3316 - val_accuracy: 0.8771
Epoch 6/10
1532/1532 [==============================] - 2s 1ms/step - loss: 0.3263 - accuracy: 0.8808 - val_loss: 0.3482 - val_accuracy: 0.8696
Epoch 7/10
1532/1532 [==============================] - 2s 1ms/step - loss: 0.3127 - accuracy: 0.8838 - val_loss: 0.3222 - val_accuracy: 0.8823
Epoch 8/10
1532/1532 [==============================] - 2s 1ms/step - loss: 0.3068 - accuracy: 0.8872 - val_loss: 0.3142 - val_accuracy: 0.8834
Epoch 9/10
1532/1532 [==============================] - 2s 1ms/step - loss: 0.2970 - accuracy: 0.8899 - val_loss: 0.3171 - val_accuracy: 0.8838
Epoch 10/10
1532/1532 [==============================] - 2s 2ms/step - loss: 0.2924 - accuracy: 0.8916 - val_loss: 0.3106 - val_accuracy: 0.8860
657/657 [==============================] - 0s 735us/step - loss: 0.3106 - accuracy: 0.8860
Test accuracy: 0.885952353477478
